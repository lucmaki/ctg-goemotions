{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0d04a7",
   "metadata": {},
   "source": [
    "# Analyse survey results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7773ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook performs statistical tests on the survey responses to evaluate soft prompts against control generated text. \n",
    "# Mainly: Chi-square tests of independence and Student's t-tests are performed to verify emotional consistency \n",
    "# and grammatical correctness respectively.\n",
    "# Means are also calculated for the observed and control groups.\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b6d7d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to survey questions and responses. \n",
    "questions = pd.read_csv(\"questions.csv\")\n",
    "responses = pd.read_csv(\"responses.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01920700",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = questions[\"EmotionSoftPrompt\"].unique().tolist()\n",
    "emotions.remove(\"none\")\n",
    "topics = questions[\"TopicPrompt\"].unique().tolist()\n",
    "responses_likert = responses.select_dtypes(include=['int', 'float'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c01a9bde",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Emotion Classification: Accuracy of each soft prompt for observed vs control ####\n",
      "amusement : 0.35526315789473684  (control: 0.2631578947368421 )\n",
      "gratitude : 0.4605263157894737  (control: 0.2236842105263158 )\n",
      "disgust : 0.4342105263157895  (control: 0.013157894736842105 )\n",
      "fear : 0.32894736842105265  (control: 0.039473684210526314 )\n",
      "surprise : 0.039473684210526314  (control: 0.09210526315789473 )\n",
      "curiosity : 0.5131578947368421  (control: 0.17105263157894737 )\n"
     ]
    }
   ],
   "source": [
    "# Measures emotion classification accuracy in responses, for given emotion soft prompt. \n",
    "# If control set to True, instead calculates accuracy on the control responses.\n",
    "def calculate_emotion_acc(emotion, control=False):\n",
    "    relevant_questions = questions[[\"EmotionSoftPrompt\", \"TopicPrompt\", \"Text\"]]\n",
    "    \n",
    "    #Filter for emotion or control entries\n",
    "    filter = relevant_questions[\"EmotionSoftPrompt\"]==emotion\n",
    "    if control:\n",
    "        filter = relevant_questions[\"EmotionSoftPrompt\"]==\"none\"\n",
    "\n",
    "    relevant_questions = relevant_questions.where(filter).dropna()\n",
    "        \n",
    "    texts = relevant_questions[\"Text\"].tolist()\n",
    "    responses_for_emotion_soft_prompt = responses[texts]\n",
    "    \n",
    "    correct = responses_for_emotion_soft_prompt[responses_for_emotion_soft_prompt==emotion].count(axis=1).sum()\n",
    "    total = responses_for_emotion_soft_prompt.count(axis=1).sum()\n",
    "    acc = correct/total\n",
    "    return acc\n",
    "\n",
    "print(\"#### Emotion Classification: Accuracy of each soft prompt for observed vs control ####\")\n",
    "for emotion in emotions:\n",
    "    acc = calculate_emotion_acc(emotion)\n",
    "    control_acc = calculate_emotion_acc(emotion, control=True)\n",
    "    print(emotion,\":\",acc,\" (control:\",control_acc, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c2e5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Emotion Classification: Chi Squared Tests ####\n",
      "amusement : p= 0.2923397756363371 , c= 1.1088145896656534\n",
      "gratitude : p= 0.0036550693609837238 , c= 8.447692307692307\n",
      "disgust : p= 1.5998144185176544e-09 , c= 36.408773678963115\n",
      "fear : p= 1.1132954059689901e-05 , c= 19.306451612903224\n",
      "surprise : p= 0.3263366851462598 , c= 0.9633802816901409\n",
      "curiosity : p= 1.917799723259888e-05 , c= 18.26923076923077\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def get_emotion_responses_series(emotion, control=False):\n",
    "    relevant_questions = questions[[\"EmotionSoftPrompt\", \"TopicPrompt\", \"Text\"]]\n",
    "    \n",
    "    # Filter for emotion and control emotion classification responses\n",
    "    filter = relevant_questions[\"EmotionSoftPrompt\"]==emotion\n",
    "    if control:\n",
    "        filter = relevant_questions[\"EmotionSoftPrompt\"]==\"none\"\n",
    "    emotion_questions = relevant_questions.where(filter).dropna()     \n",
    "    texts = emotion_questions[\"Text\"].tolist()\n",
    "    relevant_responses = responses[texts]\n",
    "\n",
    "    responses_series = pd.Series(relevant_responses.values.ravel())\n",
    "    responses_series = responses_series.mask(responses_series!=emotion, False)\n",
    "    responses_series = responses_series.mask(responses_series==emotion, True)\n",
    "    \n",
    "    return responses_series\n",
    "\n",
    "def prepare_emotion_responses_observed_control(emotion):\n",
    "    responses_observed = get_emotion_responses_series(emotion)\n",
    "    df_observed = pd.DataFrame({\"is_\"+emotion: responses_observed})\n",
    "    df_observed['is_control'] = df_observed.apply(lambda row: False, axis=1)\n",
    "\n",
    "    \n",
    "    responses_control = get_emotion_responses_series(emotion, control=True)\n",
    "    df_control = pd.DataFrame({\"is_\"+emotion: responses_control})\n",
    "    df_control['is_control'] = df_control.apply(lambda row: True, axis=1)\n",
    "                                      \n",
    "    df = pd.concat([df_observed, df_control])\n",
    "\n",
    "    return df \n",
    "                                                \n",
    "def get_contigency(emotion):\n",
    "    df = prepare_emotion_responses_observed_control(emotion)\n",
    "    contigency= pd.crosstab(df[\"is_\"+emotion], df[\"is_control\"])\n",
    "    \n",
    "    return contigency\n",
    "\n",
    "# Perform Chi Squared Test for emotion classification of the given emotion against the control responses. \n",
    "def calc_chi_sqrd(emotion):\n",
    "    contigency = get_contigency(emotion)\n",
    "    c, p, dof, expected = chi2_contingency(contigency)\n",
    "    \n",
    "    return c, p, dof, expected\n",
    "\n",
    "print(\"#### Emotion Classification: Chi Squared Tests ####\")\n",
    "for emotion in emotions:\n",
    "    c, p, dof, expected = calc_chi_sqrd(emotion)\n",
    "    print(emotion, \": p=\", p, \", c=\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb5b220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Grammatical Scores: Student's T-Tests + Means for observed and control ####\n",
      "(control mean= 3.5526315789473686 )\n",
      "amusement : p= 0.8988177932240786 , f= -0.12737045052816623 , mean= 3.5789473684210527\n",
      "gratitude : p= 0.009865156924062627 , f= 2.6138603751408107 , mean= 3.0\n",
      "disgust : p= 0.000748435766972596 , f= 3.441981590562693 , mean= 2.8421052631578947\n",
      "fear : p= 0.008500301152973247 , f= 2.666715938769339 , mean= 3.0\n",
      "surprise : p= 0.009691690823023498 , f= 2.6201982526352903 , mean= 2.986842105263158\n",
      "curiosity : p= 0.06493258193294145 , f= 1.8593771386077484 , mean= 3.1578947368421053\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def get_likert_responses_series(emotion, control=False):\n",
    "    relevant_questions = questions[[\"EmotionSoftPrompt\", \"TopicPrompt\", \"Text\"]]\n",
    "    \n",
    "    # Filter for emotion and control grammatical correctness Likert score responses\n",
    "    filter = relevant_questions[\"EmotionSoftPrompt\"]==emotion\n",
    "    if control:\n",
    "        filter = relevant_questions[\"EmotionSoftPrompt\"]==\"none\"\n",
    "    emotion_questions = relevant_questions.where(filter).dropna()     \n",
    "    texts = emotion_questions[\"Text\"].tolist()\n",
    "    texts = [e + \".1\" for e in texts]\n",
    "    relevant_responses = responses_likert[texts]\n",
    "\n",
    "    responses_series = pd.Series(relevant_responses.values.ravel())\n",
    "    \n",
    "    return responses_series\n",
    "\n",
    "def prepare_likert_responses_observed_control(emotion):\n",
    "    responses_observed = get_likert_responses_series(emotion)\n",
    "    df_observed = pd.DataFrame({\"likert_val\": responses_observed})\n",
    "    df_observed['is_control'] = df_observed.apply(lambda row: False, axis=1)\n",
    "\n",
    "    \n",
    "    responses_control = get_likert_responses_series(emotion, control=True)\n",
    "    df_control = pd.DataFrame({\"likert_val\": responses_control})\n",
    "    df_control['is_control'] = df_control.apply(lambda row: True, axis=1)\n",
    "                                      \n",
    "    df = pd.concat([df_observed, df_control])\n",
    "\n",
    "    return df \n",
    "\n",
    "# Perform t-test for Likert scale grammatical correctness responses for the given emotion, against the control responses. \n",
    "def calc_t_test(emotion):\n",
    "    df = prepare_likert_responses_observed_control(emotion)\n",
    "    return stats.ttest_ind(df['likert_val'][df['is_control'] == True], df['likert_val'][df['is_control'] == False])\n",
    "\n",
    "\n",
    "print(\"#### Grammatical Scores: Student's T-Tests + Means for observed and control ####\")\n",
    "calc_control_acc = True\n",
    "for emotion in emotions:\n",
    "    if calc_control_acc:\n",
    "        control = get_likert_responses_series(emotion, control=True)\n",
    "        print(\"(control mean=\", control.mean(), \")\")\n",
    "        calc_control_acc=False\n",
    "    observed = get_likert_responses_series(emotion)\n",
    "    t_test_result = calc_t_test(emotion)\n",
    "    print(emotion, \": p=\",t_test_result.pvalue, \", f=\", t_test_result.statistic, \", mean=\", observed.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7691a0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Normality assumption check ####\n",
      "ShapiroResult(statistic=0.9055864810943604, pvalue=5.610817793735471e-18)\n"
     ]
    }
   ],
   "source": [
    "print(\"#### Normality assumption check ####\")\n",
    "# Performs Shapiro test to verify assumption of normality for data distribution. \n",
    "def calc_normality_test():\n",
    "    all_likert_scores = np.concatenate(responses_likert.to_numpy())\n",
    "    all_likert_scores = all_likert_scores[~np.isnan(all_likert_scores)]\n",
    "    return stats.shapiro(all_likert_scores)\n",
    "\n",
    "print(calc_normality_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ba16087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Homogenity of variance assumption check ####\n",
      "LeveneResult(statistic=0.5079606851362205, pvalue=0.4762973243973677)\n"
     ]
    }
   ],
   "source": [
    "print(\"#### Homogenity of variance assumption check ####\")\n",
    "def get_all_likert_responses(control=False):\n",
    "    relevant_questions = questions[[\"EmotionSoftPrompt\", \"TopicPrompt\", \"Text\"]]\n",
    "    if control:\n",
    "        filter = relevant_questions[\"EmotionSoftPrompt\"]==\"none\"\n",
    "        relevant_questions = relevant_questions.where(filter)\n",
    "    relevant_questions = relevant_questions.dropna()\n",
    "    texts = relevant_questions[\"Text\"].tolist()\n",
    "    texts = [e + \".1\" for e in texts]\n",
    "    relevant_responses = responses_likert[texts]\n",
    "\n",
    "    responses_series = pd.Series(relevant_responses.values.ravel())\n",
    "    \n",
    "    return responses_series\n",
    "\n",
    "def prepare_all_likert_responses_observed_control():\n",
    "    responses_observed = get_all_likert_responses()\n",
    "    df_observed = pd.DataFrame({\"likert_val\": responses_observed})\n",
    "    df_observed['is_control'] = df_observed.apply(lambda row: False, axis=1)\n",
    "\n",
    "    \n",
    "    responses_control = get_all_likert_responses(control=True)\n",
    "    df_control = pd.DataFrame({\"likert_val\": responses_control})\n",
    "    df_control['is_control'] = df_control.apply(lambda row: True, axis=1)\n",
    "                                      \n",
    "    df = pd.concat([df_observed, df_control])\n",
    "\n",
    "    return df \n",
    "\n",
    "# Perform Levene test to verify assumption of data homogenity. \n",
    "def calc_var_homogenity_test():\n",
    "    df = prepare_all_likert_responses_observed_control()\n",
    "    return stats.levene(df['likert_val'][df['is_control'] == True], df['likert_val'][df['is_control'] == False])\n",
    "print(calc_var_homogenity_test())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:casual]",
   "language": "python",
   "name": "conda-env-casual-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
